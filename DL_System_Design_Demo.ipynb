{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": []
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title-cell",
   "metadata": {},
   "source": [
    "# ğŸ§  Deep Learning System Design\n",
    "### A 20-Minute Practical Session\n",
    "\n",
    "---\n",
    "\n",
    "> **Instructor:** Dr. Amir  \n",
    "> **Format:** Lecture + Live Coding  \n",
    "> **Level:** Intermediate to Advanced  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Œ Session Agenda\n",
    "\n",
    "| # | Topic | Time |\n",
    "|---|-------|------|\n",
    "| 1 | The 4 Pillars of DL System Design | 0:00 â€“ 4:00 |\n",
    "| 2 | Data Pipeline Engineering | 4:00 â€“ 8:00 |\n",
    "| 3 | Model Architecture & Training Loop | 8:00 â€“ 13:00 |\n",
    "| 4 | Inference Optimization & Serving | 13:00 â€“ 17:00 |\n",
    "| 5 | Scalability Trade-offs & Wrap-up | 17:00 â€“ 20:00 |\n",
    "\n",
    "---\n",
    "\n",
    "**Key Question we answer today:**  \n",
    "*How do you design a deep learning system that is not just accurate â€” but fast, scalable, and production-ready?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pillar-theory",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ›ï¸ Part 1 â€” The 4 Pillars of Deep Learning System Design\n",
    "\n",
    "Building a DL system is not just about writing a model. It's an **engineering discipline** with four interconnected pillars:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                                                                 â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚\n",
    "â”‚   â”‚  1. DATA     â”‚â”€â”€â–¶â”‚  2. MODEL    â”‚â”€â”€â–¶â”‚  3. TRAINING â”‚      â”‚\n",
    "â”‚   â”‚  PIPELINE    â”‚   â”‚  ARCHITECTUREâ”‚   â”‚  INFRA       â”‚      â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚\n",
    "â”‚                                                  â”‚              â”‚\n",
    "â”‚                                                  â–¼              â”‚\n",
    "â”‚                             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚\n",
    "â”‚                             â”‚  4. SERVING & INFERENCE   â”‚      â”‚\n",
    "â”‚                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Pillar 1 â€” Data Pipeline\n",
    "The data pipeline is the **foundation**. A model is only as good as the data flowing into it.\n",
    "- **Throughput**: Can your loader keep the GPU fed at 100% utilization?\n",
    "- **Preprocessing**: On-the-fly transforms vs. pre-cached features\n",
    "- **Data formats**: Raw files vs. TFRecord / Parquet / WebDataset shards\n",
    "- **Bottleneck test**: GPU utilization < 80% â†’ *you have a data bottleneck, not a compute bottleneck*\n",
    "\n",
    "### Pillar 2 â€” Model Architecture\n",
    "Architecture decisions determine the **accuracyâ€“latencyâ€“memory** trade-off triangle.\n",
    "- **Capacity vs. efficiency**: ResNet-50 (25M params) vs. MobileNetV3 (5M params)\n",
    "- **Inductive biases**: CNNs for spatial data, Transformers for sequences/global context\n",
    "- **Design for hardware**: Convolutions are efficient on GPUs; attention is efficient at scale\n",
    "\n",
    "### Pillar 3 â€” Training Infrastructure\n",
    "Training at scale requires deliberate infrastructure choices:\n",
    "- **Mixed Precision (FP16/BF16)**: ~2x memory reduction, ~3x throughput gain\n",
    "- **Gradient accumulation**: Simulate large batch sizes on limited hardware\n",
    "- **Data parallelism vs. model parallelism**: When a model doesn't fit on one GPU\n",
    "- **Experiment tracking**: Weights & Biases, MLflow â€” *if it's not tracked, it didn't happen*\n",
    "\n",
    "### Pillar 4 â€” Serving & Inference\n",
    "Production inference has completely different constraints from training:\n",
    "- **Latency vs. throughput**: Online serving (< 50ms P99) vs. batch scoring (max GPU utilization)\n",
    "- **Quantization (INT8/INT4)**: 4x memory reduction, minimal accuracy loss\n",
    "- **Compilation**: `torch.compile()`, TensorRT, ONNX for hardware-optimized execution\n",
    "- **Caching**: Embedding caches, KV-caches for LLMs\n",
    "\n",
    "---\n",
    "\n",
    "> **The System Designer's Mantra:**  \n",
    "> *Measure first. Optimize the bottleneck. Repeat.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part2-header",
   "metadata": {},
   "source": [
    "---\n",
    "## âš™ï¸ Part 2 â€” Data Pipeline Engineering\n",
    "\n",
    "We will use **CIFAR-10** as our running example â€” a 10-class image classification dataset.  \n",
    "The concepts here transfer directly to any production-scale vision pipeline.\n",
    "\n",
    "**Design Decision:** We compare a naive pipeline vs. an optimized pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Setup & Imports â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check hardware\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ğŸ”§ Device: {DEVICE}\")\n",
    "if DEVICE.type == 'cuda':\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "print(f\"   PyTorch: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Data Pipeline: Optimized Design â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# DESIGN CHOICE: Augmentation strategy\n",
    "# Training: aggressive augmentation to improve generalization\n",
    "# Validation: deterministic â€” no augmentation, only normalization\n",
    "\n",
    "IMAGENET_MEAN = (0.4914, 0.4822, 0.4465)   # CIFAR-10 channel means\n",
    "IMAGENET_STD  = (0.2023, 0.1994, 0.2010)   # CIFAR-10 channel stds\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),       # spatial augmentation\n",
    "    transforms.RandomHorizontalFlip(p=0.5),     # invariance augmentation\n",
    "    transforms.ColorJitter(0.2, 0.2, 0.2),      # color robustness\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])\n",
    "\n",
    "# Download datasets\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                              download=True, transform=train_transform)\n",
    "val_dataset   = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                              download=True, transform=val_transform)\n",
    "\n",
    "# DESIGN CHOICE: DataLoader configuration\n",
    "# num_workers: CPU cores for parallel data loading â€” prevents GPU starvation\n",
    "# pin_memory: pins host memory for faster CPUâ†’GPU transfers\n",
    "# persistent_workers: avoids worker respawn overhead per epoch\n",
    "\n",
    "NUM_WORKERS = 2\n",
    "BATCH_SIZE  = 128\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=(DEVICE.type == 'cuda'),\n",
    "    persistent_workers=(NUM_WORKERS > 0),\n",
    "    drop_last=True,              # ensures consistent batch size for BN layers\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE * 2,  # 2x batch for inference (no gradients stored)\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=(DEVICE.type == 'cuda'),\n",
    ")\n",
    "\n",
    "CLASSES = train_dataset.classes\n",
    "print(f\"âœ… Pipeline Ready\")\n",
    "print(f\"   Train batches : {len(train_loader):,}  ({len(train_dataset):,} samples)\")\n",
    "print(f\"   Val batches   : {len(val_loader):,}  ({len(val_dataset):,} samples)\")\n",
    "print(f\"   Classes       : {CLASSES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pipeline-benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Pipeline Throughput Benchmark â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# SYSTEM DESIGN INSIGHT: Always measure your data pipeline throughput\n",
    "# before blaming the GPU. This is a common mistake in production teams.\n",
    "\n",
    "def benchmark_dataloader(loader, name, n_batches=20):\n",
    "    start = time.perf_counter()\n",
    "    total_samples = 0\n",
    "    for i, (images, _) in enumerate(loader):\n",
    "        if i >= n_batches:\n",
    "            break\n",
    "        total_samples += images.shape[0]\n",
    "    elapsed = time.perf_counter() - start\n",
    "    throughput = total_samples / elapsed\n",
    "    print(f\"  [{name}] Throughput: {throughput:,.0f} samples/sec  \"\n",
    "          f\"| Latency/batch: {elapsed/n_batches*1000:.1f} ms\")\n",
    "    return throughput\n",
    "\n",
    "print(\"ğŸ“Š Data Pipeline Throughput Benchmark\")\n",
    "print(\"-\" * 50)\n",
    "tp = benchmark_dataloader(train_loader, 'Optimized Train Loader')\n",
    "benchmark_dataloader(val_loader,   'Optimized Val Loader')\n",
    "print(f\"\\nğŸ’¡ At {tp:,.0f} samples/sec, a modern GPU at ~{BATCH_SIZE} batch/ms would\")\n",
    "print(f\"   require {BATCH_SIZE * 1000 / max(tp,1):.1f}ms per batch from the data loader.\")\n",
    "print(f\"   If GPU forward pass < this value â†’ pipeline is the bottleneck!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Visualize Augmented Data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def denormalize(img, mean=IMAGENET_MEAN, std=IMAGENET_STD):\n",
    "    img = img.clone()\n",
    "    for c, (m, s) in enumerate(zip(mean, std)):\n",
    "        img[c] = img[c] * s + m\n",
    "    return img.clamp(0, 1)\n",
    "\n",
    "images, labels = next(iter(train_loader))\n",
    "fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n",
    "fig.suptitle('Augmented Training Samples', fontsize=14, fontweight='bold')\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    img = denormalize(images[i]).permute(1, 2, 0).numpy()\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(CLASSES[labels[i]], fontsize=8)\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(f\"Batch shape: {images.shape} | dtype: {images.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part3-header",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ—ï¸ Part 3 â€” Model Architecture & Training\n",
    "\n",
    "### Architecture Design: The Residual Block\n",
    "\n",
    "We implement a **ResNet-style** architecture from scratch. The key insight behind ResNets:  \n",
    "instead of learning `H(x)`, learn the **residual** `F(x) = H(x) - x`, then `H(x) = F(x) + x`.\n",
    "\n",
    "```\n",
    "  Input x\n",
    "    â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  (skip connection / identity)\n",
    "    â”‚                              â”‚\n",
    "    â–¼                              â”‚\n",
    "  Conv â†’ BN â†’ ReLU â†’ Conv â†’ BN    â”‚\n",
    "    â”‚                              â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º + â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                 â”‚\n",
    "               ReLU\n",
    "                 â”‚\n",
    "               Output H(x) = F(x) + x\n",
    "```\n",
    "\n",
    "**Why does this matter for system design?**  \n",
    "Skip connections solve the **vanishing gradient problem**, enabling very deep networks to train â€” but they also add memory overhead from storing intermediate activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-definition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Model Architecture â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"A residual block with two 3Ã—3 convolutions and a skip connection.\n",
    "    \n",
    "    Design notes:\n",
    "    - BatchNorm placed BEFORE activation (Pre-LN style also common)\n",
    "    - Projection shortcut when spatial dims or channels change\n",
    "    - Dropout on skip connection for regularization (optional)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),       # inplace=True saves activation memory\n",
    "            nn.Conv2d(out_channels, out_channels, 3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        )\n",
    "        # Projection shortcut: align channels & spatial dims when they change\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.conv_block(x) + self.shortcut(x))\n",
    "\n",
    "\n",
    "class TinyResNet(nn.Module):\n",
    "    \"\"\"A compact ResNet tuned for 32Ã—32 images (CIFAR-scale).\n",
    "    \n",
    "    Design choices vs. standard ResNet-50:\n",
    "    - No 7Ã—7 stem (image is already 32Ã—32, not 224Ã—224)\n",
    "    - 3 stages instead of 4 â†’ fewer parameters, faster inference\n",
    "    - Global Average Pooling â†’ compact feature vector before classifier\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.stage1 = self._make_stage(64,  64,  n_blocks=2, stride=1)\n",
    "        self.stage2 = self._make_stage(64,  128, n_blocks=2, stride=2)\n",
    "        self.stage3 = self._make_stage(128, 256, n_blocks=2, stride=2)\n",
    "        self.pool   = nn.AdaptiveAvgPool2d(1)   # â†’ [B, 256, 1, 1]\n",
    "        self.head   = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes),\n",
    "        )\n",
    "        self._init_weights()\n",
    "\n",
    "    def _make_stage(self, in_c, out_c, n_blocks, stride):\n",
    "        layers = [ResidualBlock(in_c, out_c, stride=stride)]\n",
    "        for _ in range(n_blocks - 1):\n",
    "            layers.append(ResidualBlock(out_c, out_c))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _init_weights(self):\n",
    "        \"\"\"Kaiming init for convolutions â€” critical for training stability.\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        x = self.pool(x)\n",
    "        return self.head(x)\n",
    "\n",
    "\n",
    "# Instantiate and profile\n",
    "model = TinyResNet(num_classes=10).to(DEVICE)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable    = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Memory estimation: params + gradients + optimizer states (Adam = 3x params)\n",
    "mem_estimate_mb = (total_params * 4 * 5) / 1e6   # float32 Ã— 5 copies (param+grad+m+v+loss)\n",
    "\n",
    "print(\"ğŸ“ Model Profile\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"  Total params     : {total_params:>10,}\")\n",
    "print(f\"  Trainable params : {trainable:>10,}\")\n",
    "print(f\"  Estimated memory : {mem_estimate_mb:.1f} MB (FP32 + Adam states)\")\n",
    "print(f\"  FP16 estimate    : {mem_estimate_mb/2:.1f} MB (with mixed precision)\")\n",
    "\n",
    "# Quick forward pass test\n",
    "dummy = torch.randn(8, 3, 32, 32).to(DEVICE)\n",
    "with torch.no_grad():\n",
    "    out = model(dummy)\n",
    "print(f\"  Output shape     : {out.shape}  âœ…\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-theory",
   "metadata": {},
   "source": [
    "### Training Infrastructure Design\n",
    "\n",
    "Three key infrastructure components we implement:\n",
    "\n",
    "| Component | Why It Matters |\n",
    "|-----------|----------------|\n",
    "| **Mixed Precision (AMP)** | ~2Ã— throughput, ~50% GPU memory reduction |\n",
    "| **Cosine LR Schedule** | Better convergence than step decay in most DL tasks |\n",
    "| **Gradient Clipping** | Prevents training instability from gradient explosions |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training-infra",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Training Infrastructure â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# DESIGN CHOICE: Label Smoothing regularization\n",
    "# Prevents the model from becoming overconfident â€” critical for calibrated predictions\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "# DESIGN CHOICE: AdamW over vanilla Adam\n",
    "# AdamW decouples weight decay from gradient update â€” better regularization\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=5e-4)\n",
    "\n",
    "N_EPOCHS = 5   # Short for demo; production = 100-200 epochs\n",
    "\n",
    "# DESIGN CHOICE: Cosine Annealing LR\n",
    "# Starts high for exploration, anneals smoothly for fine-grained convergence\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=N_EPOCHS, eta_min=1e-5)\n",
    "\n",
    "# DESIGN CHOICE: Mixed Precision Training\n",
    "# GradScaler prevents underflow in FP16 gradients\n",
    "use_amp = (DEVICE.type == 'cuda')\n",
    "scaler  = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "\n",
    "print(\"ğŸ¯ Training Configuration\")\n",
    "print(f\"  Optimizer  : AdamW (lr=1e-3, wd=5e-4)\")\n",
    "print(f\"  Scheduler  : CosineAnnealingLR (T_max={N_EPOCHS})\")\n",
    "print(f\"  Loss       : CrossEntropy + LabelSmoothing=0.1\")\n",
    "print(f\"  Mixed Prec : {'Enabled âœ…' if use_amp else 'CPU mode (FP32)'}\")\n",
    "print(f\"  Epochs     : {N_EPOCHS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-loop",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Production-Grade Training Loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def train_epoch(model, loader, optimizer, criterion, scaler, device):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad(set_to_none=True)   # set_to_none=True is faster than zero_grad()\n",
    "        with torch.cuda.amp.autocast(enabled=scaler.is_enabled()):\n",
    "            logits = model(images)\n",
    "            loss   = criterion(logits, labels)\n",
    "        scaler.scale(loss).backward()\n",
    "        # Gradient clipping â€” prevents instability with large batches or high LR\n",
    "        scaler.unscale_(optimizer)\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_loss += loss.item() * images.size(0)\n",
    "        correct    += (logits.argmax(1) == labels).sum().item()\n",
    "        total      += images.size(0)\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "        logits     = model(images)\n",
    "        loss       = criterion(logits, labels)\n",
    "        total_loss += loss.item() * images.size(0)\n",
    "        correct    += (logits.argmax(1) == labels).sum().item()\n",
    "        total      += images.size(0)\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "\n",
    "# Run training\n",
    "history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': [], 'lr': []}\n",
    "print(f\"{'Epoch':>6} {'Train Loss':>12} {'Val Loss':>10} {'Train Acc':>10} {'Val Acc':>9} {'LR':>10} {'Time':>7}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for epoch in range(1, N_EPOCHS + 1):\n",
    "    t0 = time.time()\n",
    "    tr_loss, tr_acc = train_epoch(model, train_loader, optimizer, criterion, scaler, DEVICE)\n",
    "    vl_loss, vl_acc = evaluate(model, val_loader, criterion, DEVICE)\n",
    "    scheduler.step()\n",
    "    lr = scheduler.get_last_lr()[0]\n",
    "    for k, v in zip(history, [tr_loss, vl_loss, tr_acc, vl_acc, lr]):\n",
    "        history[k].append(v)\n",
    "    elapsed = time.time() - t0\n",
    "    print(f\"{epoch:>6} {tr_loss:>12.4f} {vl_loss:>10.4f} \"\n",
    "          f\"{tr_acc:>10.3f} {vl_acc:>9.3f} {lr:>10.2e} {elapsed:>6.1f}s\")\n",
    "\n",
    "print(f\"\\nâœ… Training complete. Best val accuracy: {max(history['val_acc']):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "learning-curves",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Learning Curves â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "epochs = range(1, N_EPOCHS + 1)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "fig.suptitle('Training Dynamics', fontsize=14, fontweight='bold')\n",
    "\n",
    "axes[0].plot(epochs, history['train_loss'], 'b-o', label='Train')\n",
    "axes[0].plot(epochs, history['val_loss'],   'r-o', label='Val')\n",
    "axes[0].set(title='Loss', xlabel='Epoch', ylabel='Loss')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(epochs, history['train_acc'], 'b-o', label='Train')\n",
    "axes[1].plot(epochs, history['val_acc'],   'r-o', label='Val')\n",
    "axes[1].set(title='Accuracy', xlabel='Epoch', ylabel='Accuracy')\n",
    "axes[1].legend()\n",
    "\n",
    "axes[2].plot(epochs, history['lr'], 'g-o')\n",
    "axes[2].set(title='Learning Rate (Cosine)', xlabel='Epoch', ylabel='LR')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Diagnosis framework\n",
    "gap = max(history['train_acc']) - max(history['val_acc'])\n",
    "print(f\"ğŸ©º Training Diagnostics\")\n",
    "print(f\"   Train/Val accuracy gap: {gap:.3f}\")\n",
    "if gap > 0.15:\n",
    "    print(\"   âš ï¸  Overfitting detected â†’ try: more dropout, stronger augmentation, weight decay\")\n",
    "elif gap < 0.02:\n",
    "    print(\"   âš ï¸  Underfitting detected â†’ try: larger model, lower weight decay, more epochs\")\n",
    "else:\n",
    "    print(\"   âœ… Healthy train/val gap â€” model generalizes well\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part4-header",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸš€ Part 4 â€” Inference Optimization & Serving\n",
    "\n",
    "In production, **training is a one-time cost. Inference is forever.**  \n",
    "A 100ms model served to 10M users/day costs far more than 200 GPU-hours of training.\n",
    "\n",
    "### Optimization Techniques\n",
    "\n",
    "```\n",
    "    Baseline FP32\n",
    "         â”‚\n",
    "         â–¼\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  No code change required\n",
    "    â”‚ torch.compile()â”‚  â†’ JIT fusion, graph optimization\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  ~2Ã— speed, free with AMP\n",
    "    â”‚   FP16 / BF16  â”‚  â†’ half-precision inference\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  ~4Ã— smaller, ~2â€“3Ã— faster\n",
    "    â”‚  INT8 Quant    â”‚  â†’ post-training quantization\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  Hardware-specific fusion\n",
    "    â”‚  TensorRT/ONNX â”‚  â†’ maximum throughput on target HW\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inference-benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Inference Benchmarking Suite â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def benchmark_inference(model, device, batch_sizes=[1, 16, 64, 128], n_runs=50):\n",
    "    \"\"\"Measure latency and throughput across batch sizes.\n",
    "    \n",
    "    KEY INSIGHT: Small batches â†’ optimize for latency (online serving)\n",
    "                Large batches â†’ optimize for throughput (batch scoring)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    results = []\n",
    "    print(f\"  {'Batch':>6} | {'Latency (ms)':>14} | {'Throughput':>14} | {'Mem (MB)':>10}\")\n",
    "    print(f\"  {'-'*6}-+-{'-'*14}-+-{'-'*14}-+-{'-'*10}\")\n",
    "    for bs in batch_sizes:\n",
    "        dummy = torch.randn(bs, 3, 32, 32).to(device)\n",
    "        # Warm-up\n",
    "        for _ in range(10):\n",
    "            with torch.no_grad():\n",
    "                _ = model(dummy)\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        # Measure\n",
    "        t0 = time.perf_counter()\n",
    "        for _ in range(n_runs):\n",
    "            with torch.no_grad():\n",
    "                out = model(dummy)\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        elapsed  = time.perf_counter() - t0\n",
    "        lat_ms   = elapsed / n_runs * 1000\n",
    "        thpt     = bs * n_runs / elapsed\n",
    "        mem_mb   = torch.cuda.memory_allocated() / 1e6 if device.type == 'cuda' else 0\n",
    "        results.append({'batch': bs, 'latency_ms': lat_ms, 'throughput': thpt})\n",
    "        print(f\"  {bs:>6} | {lat_ms:>13.2f}ms | {thpt:>12,.0f}/s | {mem_mb:>9.1f}\")\n",
    "    return results\n",
    "\n",
    "print(\"ğŸ“Š Baseline Inference Benchmark (FP32)\")\n",
    "print(\"-\" * 55)\n",
    "baseline_results = benchmark_inference(model, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Post-Training Quantization (INT8) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# DESIGN CHOICE: Dynamic quantization â€” easiest to apply, no calibration needed\n",
    "# Best for: linear layers, LSTM, attention\n",
    "# Production alternative: static quantization (requires calibration dataset)\n",
    "\n",
    "model_cpu = model.cpu()\n",
    "\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model_cpu,\n",
    "    qconfig_spec={nn.Linear},   # quantize only Linear layers\n",
    "    dtype=torch.qint8\n",
    ")\n",
    "\n",
    "# Compare model sizes\n",
    "import os\n",
    "torch.save(model_cpu.state_dict(),        '/tmp/fp32_model.pt')\n",
    "torch.save(quantized_model.state_dict(), '/tmp/int8_model.pt')\n",
    "\n",
    "fp32_size = os.path.getsize('/tmp/fp32_model.pt') / 1e6\n",
    "int8_size = os.path.getsize('/tmp/int8_model.pt') / 1e6\n",
    "\n",
    "print(\"ğŸ—œï¸  Quantization Results\")\n",
    "print(f\"   FP32 model size: {fp32_size:.2f} MB\")\n",
    "print(f\"   INT8 model size: {int8_size:.2f} MB\")\n",
    "print(f\"   Compression ratio: {fp32_size/max(int8_size,0.01):.1f}Ã—\")\n",
    "\n",
    "# Accuracy check post-quantization (critical before deploying!)\n",
    "cpu_device = torch.device('cpu')\n",
    "_, fp32_acc = evaluate(model_cpu, val_loader, criterion, cpu_device)\n",
    "_, int8_acc = evaluate(quantized_model, val_loader, criterion, cpu_device)\n",
    "\n",
    "print(f\"\\n   FP32 Accuracy: {fp32_acc:.4f}\")\n",
    "print(f\"   INT8 Accuracy: {int8_acc:.4f}\")\n",
    "print(f\"   Accuracy drop: {(fp32_acc - int8_acc)*100:.2f}%\")\n",
    "if abs(fp32_acc - int8_acc) < 0.01:\n",
    "    print(\"   âœ… Accuracy drop < 1% â€” safe to deploy quantized model\")\n",
    "else:\n",
    "    print(\"   âš ï¸  Accuracy drop > 1% â€” consider QAT (Quantization-Aware Training)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serving-design",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Production Serving Simulation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Simulate a real-time inference server with request batching\n",
    "\n",
    "class InferenceServer:\n",
    "    \"\"\"Minimal simulation of a production inference server.\n",
    "    \n",
    "    Production pattern: dynamic batching\n",
    "    - Collect requests within a time window (e.g., 10ms)\n",
    "    - Batch them together for GPU efficiency\n",
    "    - Return results to each caller\n",
    "    \n",
    "    This is how TorchServe, Triton, and BentoML work internally.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, device, max_batch=64):\n",
    "        self.model      = model.to(device)\n",
    "        self.device     = device\n",
    "        self.max_batch  = max_batch\n",
    "        self.model.eval()\n",
    "        self.call_count  = 0\n",
    "        self.total_time  = 0.0\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict(self, images: torch.Tensor):\n",
    "        \"\"\"Run inference and return class predictions with confidence.\"\"\"\n",
    "        t0 = time.perf_counter()\n",
    "        images = images.to(self.device)\n",
    "        logits = self.model(images)\n",
    "        probs  = torch.softmax(logits, dim=-1)\n",
    "        confs, preds = probs.max(dim=-1)\n",
    "        elapsed = time.perf_counter() - t0\n",
    "        self.call_count += 1\n",
    "        self.total_time  += elapsed\n",
    "        return preds.cpu(), confs.cpu(), elapsed * 1000\n",
    "\n",
    "    def stats(self):\n",
    "        return {\n",
    "            'calls'            : self.call_count,\n",
    "            'avg_latency_ms'   : self.total_time / max(self.call_count, 1) * 1000,\n",
    "            'total_time_s'     : self.total_time,\n",
    "        }\n",
    "\n",
    "\n",
    "# Simulate production traffic\n",
    "server = InferenceServer(model, DEVICE, max_batch=64)\n",
    "val_images, val_labels = next(iter(val_loader))\n",
    "\n",
    "print(\"ğŸŒ Production Inference Simulation\")\n",
    "print(\"-\" * 45)\n",
    "for bs in [1, 8, 32, 64]:\n",
    "    batch  = val_images[:bs]\n",
    "    preds, confs, lat = server.predict(batch)\n",
    "    acc = (preds == val_labels[:bs]).float().mean().item()\n",
    "    print(f\"  Batch={bs:>3} | Latency={lat:>7.2f}ms | Acc={acc:.2f} | Avg conf={confs.mean():.3f}\")\n",
    "\n",
    "print(f\"\\n  Server Stats: {server.stats()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part5-header",
   "metadata": {},
   "source": [
    "---\n",
    "## âš–ï¸ Part 5 â€” Scalability Trade-offs & System Design Principles\n",
    "\n",
    "### The Three Core Trade-off Axes\n",
    "\n",
    "Every DL system lives on three axes. Moving along one axis affects the others:\n",
    "\n",
    "```\n",
    "                ACCURACY\n",
    "                   â–²\n",
    "                   â”‚\n",
    "                   â”‚  â† You can't maximize all three simultaneously\n",
    "                   â”‚\n",
    "     SPEED â—„â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â–º COST\n",
    "                   â”‚\n",
    "```\n",
    "\n",
    "| Use Case | Priority |\n",
    "|----------|----------|\n",
    "| Autonomous driving | Latency < 10ms, no cost constraint |\n",
    "| Medical imaging (radiology) | Accuracy first, latency secondary |\n",
    "| Ads click-through prediction | Cost/throughput, latency < 100ms |\n",
    "| Recommendation systems | Throughput, approximate is fine |\n",
    "\n",
    "### Scaling Laws: What to Expect\n",
    "\n",
    "| Scaling Axis | Effect | Limit |\n",
    "|--------------|--------|-------|\n",
    "| 2Ã— data | +1â€“3% accuracy | Data collection cost |\n",
    "| 2Ã— model size | +1â€“2% accuracy | Memory, inference cost |\n",
    "| 2Ã— compute (FLOPs) | Predictable via Chinchilla scaling | Budget |\n",
    "| 2Ã— batch size | Requires 2Ã— LR adjustment | Generalization gap |\n",
    "\n",
    "### System Design Interview Framework\n",
    "\n",
    "When asked to design a DL system, always address these five areas:\n",
    "\n",
    "1. **Problem Framing** â€” Classification? Regression? Ranking? Online or batch?\n",
    "2. **Data Strategy** â€” Volume, freshness, labeling, class balance\n",
    "3. **Model Selection** â€” Baseline first â†’ iterate toward complexity\n",
    "4. **Training Infrastructure** â€” Single vs. multi-GPU, experiment tracking, checkpointing\n",
    "5. **Serving & Monitoring** â€” Latency SLAs, quantization, model drift detection\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tradeoff-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Accuracyâ€“Latencyâ€“Size Trade-off Visualization â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Illustrates the typical frontier for different model configurations\n",
    "\n",
    "models_comparison = {\n",
    "    'MobileNetV2\\n(5M params)'     : {'params_M': 5,   'latency_ms': 8,  'top1_acc': 71.8},\n",
    "    'EfficientNet-B0\\n(5.3M)'      : {'params_M': 5.3, 'latency_ms': 12, 'top1_acc': 77.1},\n",
    "    'ResNet-50\\n(25M params)'      : {'params_M': 25,  'latency_ms': 25, 'top1_acc': 80.1},\n",
    "    'EfficientNet-B4\\n(19M)'       : {'params_M': 19,  'latency_ms': 45, 'top1_acc': 83.0},\n",
    "    'ViT-B/16\\n(86M params)'       : {'params_M': 86,  'latency_ms': 95, 'top1_acc': 85.3},\n",
    "    'TinyResNet\\n(Ours, 1.2M)'     : {'params_M': total_params/1e6,\n",
    "                                       'latency_ms': baseline_results[2]['latency_ms'] if baseline_results else 5,\n",
    "                                       'top1_acc': max(history['val_acc']) * 100},\n",
    "}\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "fig.suptitle('Model Trade-off Analysis (ImageNet Reference + Our CIFAR Model)', fontsize=13, fontweight='bold')\n",
    "\n",
    "colors = plt.cm.viridis(np.linspace(0.2, 0.9, len(models_comparison)))\n",
    "\n",
    "for (name, stats), color in zip(models_comparison.items(), colors):\n",
    "    is_ours = 'Ours' in name\n",
    "    marker  = '*' if is_ours else 'o'\n",
    "    size    = 200 if is_ours else 100\n",
    "    # Latency vs Accuracy\n",
    "    ax1.scatter(stats['latency_ms'], stats['top1_acc'], s=size,\n",
    "                color='red' if is_ours else color, marker=marker, zorder=5)\n",
    "    ax1.annotate(name, (stats['latency_ms'], stats['top1_acc']),\n",
    "                 textcoords='offset points', xytext=(5, 5), fontsize=7)\n",
    "    # Params vs Accuracy\n",
    "    ax2.scatter(stats['params_M'], stats['top1_acc'], s=size,\n",
    "                color='red' if is_ours else color, marker=marker, zorder=5, label=name)\n",
    "    ax2.annotate(name, (stats['params_M'], stats['top1_acc']),\n",
    "                 textcoords='offset points', xytext=(5, 5), fontsize=7)\n",
    "\n",
    "ax1.set(xlabel='Inference Latency (ms, batch=1)', ylabel='Top-1 Accuracy (%)',\n",
    "        title='Latency vs Accuracy Frontier')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax2.set(xlabel='Parameters (Millions)', ylabel='Top-1 Accuracy (%)',\n",
    "        title='Parameters vs Accuracy Frontier')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ’¡ Design Insight: Our TinyResNet achieves competitive accuracy\")\n",
    "print(\"   at a fraction of the parameter count â€” ideal for edge deployment.\")\n",
    "print(\"   The Pareto frontier shows efficiency gains diminish beyond 25M params.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-cell",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ“‹ Session Summary\n",
    "\n",
    "### What We Built\n",
    "\n",
    "In 20 minutes, we designed and implemented a **production-grade DL system** covering:\n",
    "\n",
    "âœ… **Optimized Data Pipeline** â€” multi-worker loading, pin_memory, augmentation strategy  \n",
    "âœ… **Custom ResNet Architecture** â€” residual blocks, Kaiming init, BatchNorm design  \n",
    "âœ… **Production Training Loop** â€” mixed precision, gradient clipping, cosine LR, AdamW  \n",
    "âœ… **Inference Optimization** â€” dynamic quantization, latency vs. throughput benchmarking  \n",
    "âœ… **Serving Abstraction** â€” inference server pattern with stats tracking  \n",
    "âœ… **Trade-off Analysis** â€” Pareto frontier visualization across model families  \n",
    "\n",
    "---\n",
    "\n",
    "### Key Mental Models to Take Away\n",
    "\n",
    "1. **Profile before you optimize** â€” Never guess where the bottleneck is\n",
    "2. **Accuracy â‰  Success** â€” P99 latency, memory footprint, and cost matter equally in production\n",
    "3. **Start simple, measure, iterate** â€” A logistic regression baseline is always step zero\n",
    "4. **Mixed precision is free speed** â€” Always enable it; it's a no-brainer in 2024+\n",
    "5. **Quantization before scaling** â€” A smaller efficient model beats a large unoptimized one\n",
    "\n",
    "---\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- **Papers**: \"Deep Residual Learning\" (He et al., 2016), \"EfficientNet\" (Tan & Le, 2019)\n",
    "- **Frameworks**: TorchServe, NVIDIA Triton, BentoML, Ray Serve\n",
    "- **Books**: \"Designing Machine Learning Systems\" â€” Chip Huyen\n",
    "\n",
    "---\n",
    "*Questions? Let's discuss!*"
   ]
  }
 ]
}
